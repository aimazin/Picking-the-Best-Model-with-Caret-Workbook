{"cells":[{"metadata":{"_uuid":"db29a2ba5ef44f0b6f3019cef7a6633684bd2d98","_cell_guid":"51c0fb5b-8bb3-4afa-bf97-430dd32f59eb"},"cell_type":"markdown","source":"# Introduction\n\nThis is the companion workbook to [Picking the Best Model with Caret](https://www.kaggle.com/rtatman/picking-the-best-model-with-caret). Read through the lesson and then come here to complete the exercises so all your work is in one central place.\n\nFor this workbook we'll be working with a dataset of FIFA (football/soccer) players from EASports' FIFA video game series. You'll be predicting what a player's rank will be given their other attributes.\n____\n\n**Remember**: If you want to share this notebook you need to make it public so that other people can see it. You can do that by forking this notebook and then selecting \"public\" on the drop-down menu to the left of the \"Publish\" button.\n____\n# Table of Contents: \n\n* [Setting up our environment](#Setting-up-our-environment)\n* [Clean our data](#Clean-our-data)\n* [Split data into testing & training](#Split-data-into-testing-&-training)\n* [Fit a baseline model](#Fit-a-baseline-model)\n* [Tune a model using caret](#Tune-a-model-using-caret)\n* [Compare our models](#Compare-our-models)"},{"metadata":{"_uuid":"9c666821d52e3fefa6c4b117ed942aa13f2853cd","_cell_guid":"7abef454-9168-4893-ab7e-988f3d531692"},"cell_type":"markdown","source":"# Setting up our environment\n___\n\nHere I've set up the environment for you. Remember to run this cell first or the other cells won't work! :)"},{"metadata":{"_kg_hide-output":true,"_uuid":"248ef4550532c83ac2fbf788f95c819fb5b2ab67","trusted":true,"_cell_guid":"c56cb8e1-1a06-4f31-a408-54a40128339c"},"cell_type":"code","source":"# libraries we'll use\nlibrary(tidyverse) # utility functions\nlibrary(caret) # hyperparameter tuning\nlibrary(randomForest)\nlibrary(Metrics) #useful metrics\n\n# read in data\nplayer_statistics <- read_csv(\"../input/FullData.csv\")","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"fe1fbe1d60f086d11c4731506f0ca59e1dd264fc","_cell_guid":"4f6a0137-2b1f-4b50-864c-5ca9e80528fd"},"cell_type":"markdown","source":"# Clean our data\n___\n\nThis datset is pretty clean. All you'll need to do is remove rows with na's and select just the numeric columns."},{"metadata":{"_uuid":"87429837c7f630f025b16ca6b752c9e17cf0cca1","trusted":true,"_cell_guid":"54699973-cb5e-4783-87e0-219df12398c8"},"cell_type":"code","source":"# omit na's and remove non-numeric columns\nplayer_statistics <- player_statistics %>%\nna.omit() %>%\nselect_if(is.numeric)\n","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"9d9d6662678ba0e8b4ec772a18e573dc3d60f353","_cell_guid":"d140d501-1a36-476b-8af6-ce28d836c66a"},"cell_type":"markdown","source":"Check your dataframe out to make sure it looks reasonable. "},{"metadata":{"_uuid":"7c62a7743fcecfa23ace67f794c7905bf21ff6a3","trusted":true,"_cell_guid":"f2d6babc-9fa3-423e-aa10-c269483bf422"},"cell_type":"code","source":"# check out your data frame using the str() function\nstr(player_statistics)\n","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"5d80e438ea11133ef07bbe63976a739df7dd251a","_cell_guid":"b3fca0d7-58f8-4ef8-a9d5-412c5db2ddcd"},"cell_type":"markdown","source":"# Split data into testing & training\n____\n\nSplit your data so that 80% of your data in the training set and 20% is in the testing set. "},{"metadata":{"_uuid":"f521cc8aa06518852200bfec943d6d89bcfc40ba","trusted":true,"_cell_guid":"ca9da8e9-bac0-4e1b-8fc5-db89697781b1"},"cell_type":"code","source":"# set a random seed\nset.seed(1234)\n\n# 80/20 train/test split\n# train/test split\ntraining_indexs <- createDataPartition(player_statistics$Rating, p = .2, list = F)\ntraining <- player_statistics[training_indexs, ]\ntesting  <- player_statistics[-training_indexs, ]\n","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"8aafb5fbadad3d4721a72907f6279a361a944a19","_cell_guid":"2021e71d-649b-4fe6-a0ae-f52094418871"},"cell_type":"markdown","source":"Convert your predictors into a matrix & get a vector of your target variables."},{"metadata":{"_uuid":"601f80b3f8c9794a2b260a94e47d09fd7a65e168","trusted":true,"_cell_guid":"32c303c7-7871-4a82-982d-06bca6a72d6b"},"cell_type":"code","source":"# get a mtrix of predictors and a vector of our target variable\npredictors <- training %>% select(-Rating) %>% as.matrix()\noutput <- training$Rating\n\n","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"17f8253a5ac07cf32180479ddc57f60d23e3f238","_cell_guid":"0e6f457f-25b7-45de-9908-cb3acd18b39b"},"cell_type":"markdown","source":"# Fit a baseline model\n____\n\nFit a baseline model using randomForest. I'd recommend setting \"ntree\" to 25.\n\n> **How can I figure out what a good ntree is?** You can check the output of a random tree model as it adds more trees by setting the argument do.trace to TRUE. It will print out the the mean standard error and what percent of the variance your model doesn't explain for each number of trees. You can then pick a ntree that is near the \"elbow\", the point at which adding additional another three stops dramatically improving your model's fit. \n\nOnce you've trained a model, you can examine it by calling the varaible you assinged it to. (So if you called your model \"base_model\", you can look at your model by running a line that's just \"base_model\".)"},{"metadata":{"_uuid":"da9b94b01008b3ce428d756ef92c2a570b45376d","trusted":true,"_cell_guid":"58a62a8b-b88a-4d87-a4d5-9620bbf46746"},"cell_type":"code","source":"# fit a model\nbase_model <- randomForest(x = predictors, y = output,\n                      ntree = 50) # number of trees\n\n# examine your model\nbase_model\n","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"6ce38a7d8d77af6ddda36dcecba4322e663e63df","_cell_guid":"e8877878-12fa-407c-893b-53c6864d87ee"},"cell_type":"markdown","source":"Finally, go ahead and calcuate the rmse (root mean squared error) for your base model on your held-out test data."},{"metadata":{"_uuid":"79e7a5c75bce5c2de7c517ebcb289941db6a1840","trusted":true,"_cell_guid":"e4c492cb-94be-4eca-91d4-fdf74f6a74c4"},"cell_type":"code","source":"# find the rmse on our test data\nrmse(predict(base_model, testing), testing$Rating)\n","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"f061dcb821b45292df8b27e9c45bdf7e0a85b926","_cell_guid":"0e6ad4fd-17cc-4110-a5f1-041ad780ee86"},"cell_type":"markdown","source":"# Tune a model using caret\n____\n\nNow that you have a base model to compare it with, try tuning the model using the \"train\" function from the caret package. You can examine your model by printing the variable you assigned your model to using the print() function. "},{"metadata":{"_uuid":"f983e1854dc6cf2cd73330133643e71c967ddc9a","trusted":true,"_cell_guid":"96d9b37c-7289-4e56-8e05-ad3e6864a47f"},"cell_type":"code","source":"# tune a model\ntuned_model <- train(x = predictors, y = output,\n                     ntree = 5, # number of trees (passed ot random forest)\n                     method = \"rf\") # random forests\n\n# examine your tuned model\nprint(tuned_model)","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"29897c2dc2ae85ba868796927f57c5d9a909d8b7","_cell_guid":"d4f746b4-11ab-407a-a9a3-b8c7bcc4afb8"},"cell_type":"markdown","source":"You can also check out the error over the different mtry values that caret tried by passing the model to the ggplot function. "},{"metadata":{"_uuid":"7ced8a840f89a0f49e811c5c6ad3ed55b9baef14","trusted":true,"_cell_guid":"fdfe5fba-0cff-4bf7-b21e-2ed3e024bb8b"},"cell_type":"code","source":"# plot the error over various mtry variables \nggplot(tuned_model)","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"1e421a4b1dff13aca814ea71ec688ded7fc0eb08","_cell_guid":"4cd841a0-4754-40d5-ac66-9a7af49d3133"},"cell_type":"markdown","source":"# Compare our models\n___\n\nNow that we have our two models, let's compare them to each other.\n\n> **Tip:** You can access the automatically-picked best model by getting the finalModel component for your tuned model. So if you called your tuned model ```model_tuned```, the best model would be ```model_tuned$finalModel```.\n\nFirst, compare the root mean squared error (rmse) for each of your models on the test data. Which model has a lower overall error on the test data? Why might this be?"},{"metadata":{"_uuid":"7758d8f6f9c4a8327bfc318a20004cfaa82940d2","trusted":true,"_cell_guid":"e49cecaf-cce0-4713-af44-b57f7af13e2a"},"cell_type":"code","source":"# get rmse for the base model on the testing data\nprint(\"Base model mean error:\")\nprint(rmse(predict(base_model, testing), testing$Rating))\n\n# get rmse for the tuned model on the testing data\nprint(\"Tuned model mean error:\")\nprint(rmse(predict(tuned_model$finalModel, testing), testing$Rating))","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"3f08994de51836f46c62966c3a6d78935374e667","_cell_guid":"43052d1c-4de3-4f4a-9aaf-9756fd01f6ab"},"cell_type":"markdown","source":"Second, look at the five most important varibles for each model. Are the the same?"},{"metadata":{"_uuid":"beae6a20420833c8ad18191d1289f68bfb389e90","trusted":true,"_cell_guid":"a768d8d6-11ed-4c35-a7dd-465f1169c8d9"},"cell_type":"code","source":"# plot the relative variable importance for our tune & un-tuned models\n\n# two columns, 1 row (for plots)\npar(mfrow = c(1,2))\n\n# plot both variable importances\nvarImpPlot(base_model, n.var = 5)\nvarImpPlot(tuned_model$finalModel, n.var = 5)\n","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"6b5352e82e77c804b588cfe72502ad6a142e1207","_cell_guid":"26ac4e51-5c07-4383-b0e4-f07adaa5a739"},"cell_type":"markdown","source":"# And that's it! :)\n___\n\nNice work! Now that you've got some practice, why not try using caret to tune a different model, like xgboost? You can check out an [R xgboost tutorial here](https://www.kaggle.com/rtatman/machine-learning-with-xgboost-in-r).\n\nHappy analyzing!"}],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"}},"nbformat":4,"nbformat_minor":1}